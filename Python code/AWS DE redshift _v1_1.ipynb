{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af99fe37",
   "metadata": {},
   "source": [
    "# Streamlining Data Processing with AWS Redshift, Python, and S3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69595f8a",
   "metadata": {},
   "source": [
    "##### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebae10df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import json\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e6972b",
   "metadata": {},
   "source": [
    "####   Read AWS and Redshift cluster configuration from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42ac2e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('cluster.config'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50266ad3",
   "metadata": {},
   "source": [
    "##### Extract AWS credentials and Redshift cluster details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9f0c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY = config.get('AWS','KEY')\n",
    "SECRET = config.get('AWS','SECRET')\n",
    "DWH_CLUSTER_TYPE = config.get('DWH','DWH_CLUSTER_TYPE')\n",
    "DWH_NUM_NODES = config.get('DWH','DWH_NUM_NODES') \n",
    "DWH_NODE_TYPE = config.get('DWH','DWH_NODE_TYPE')  \n",
    "DWH_CLUSTER_IDENTIFIER = config.get('DWH','DWH_CLUSTER_IDENTIFIER')\n",
    "DWH_DB = config.get('DWH','DWH_DB')\n",
    "DWH_DB_USER = config.get('DWH','DWH_DB_USER')\n",
    "DWH_DB_PASSWORD = config.get('DWH','DWH_DB_PASSWORD')\n",
    "DWH_PORT = config.get('DWH','DWH_PORT')\n",
    "DWH_IAM_ROLE_NAME = config.get('DWH','DWH_IAM_ROLE_NAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555548f0",
   "metadata": {},
   "source": [
    "##### Create a DataFrame to store parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2b2b97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"Param\":[\"key\" ,\"SECRET\" ,\"DWH_CLUSTER_TYPE\" ,\"DWH_NUM_NODES\" ,\"DWH_NODE_TYPE\" ,\"DWH_CLUSTER_IDENTIFIER\" ,\"DWH_DB\" ,\"DWH_DB_USER\" ,\"DWH_DB_PASSWORD\" ,\"DWH_PORT\" ,\"DWH_IAM_ROLE_NAME\"]\n",
    "              , \"Value\" : [KEY ,SECRET ,DWH_CLUSTER_TYPE ,DWH_NUM_NODES ,DWH_NODE_TYPE ,DWH_CLUSTER_IDENTIFIER ,DWH_DB,DWH_DB_USER,DWH_DB_PASSWORD,DWH_PORT ,DWH_IAM_ROLE_NAME]\n",
    "    \n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb87f9eb",
   "metadata": {},
   "source": [
    "##### Create AWS clients using Boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e79f711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ec2 = boto3.resource('ec2', region_name=\"ap-south-1\", aws_access_key_id=KEY, aws_secret_access_key=SECRET)\n",
    "s3 = boto3.resource('s3', region_name=\"ap-south-1\", aws_access_key_id=KEY, aws_secret_access_key=SECRET)\n",
    "iam_client = boto3.resource('iam', region_name=\"ap-south-1\", aws_access_key_id=KEY, aws_secret_access_key=SECRET)\n",
    "redshift_client = boto3.client('redshift', region_name=\"ap-south-1\", aws_access_key_id=KEY, aws_secret_access_key=SECRET)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56185800",
   "metadata": {},
   "source": [
    "#### Access an S3 bucket and retrieve file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "977197a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Customer.csv',\n",
       " 'Sales_date.csv',\n",
       " 'markets.csv',\n",
       " 'products.csv',\n",
       " 'transactions.csv']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bucket = s3.Bucket(\"abhinav-de-s3\")\n",
    "log_data_files = [filename.key for filename in bucket.objects.filter(Prefix='')]\n",
    "log_data_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cd9007",
   "metadata": {},
   "source": [
    "##### Create Redshift cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29f9d2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "roleArn = 'arn:aws:iam::533267202542:role/redshift-s3-role'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec915ee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster creation request submitted successfully.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = redshift_client.create_cluster(\n",
    "        ClusterType=DWH_CLUSTER_TYPE,\n",
    "        NodeType=DWH_NODE_TYPE,\n",
    "        \n",
    "        # Identifiers & credentials\n",
    "        DBName=DWH_DB,\n",
    "        ClusterIdentifier=DWH_CLUSTER_IDENTIFIER,\n",
    "        MasterUsername=DWH_DB_USER,\n",
    "        MasterUserPassword=DWH_DB_PASSWORD,\n",
    "        \n",
    "        # Role (for S3 access)\n",
    "        IamRoles=[roleArn]  # Corrected parameter name to IamRoles\n",
    "    )\n",
    "    print(\"Cluster creation request submitted successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating cluster: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1d0d3a",
   "metadata": {},
   "source": [
    "##### Function to display selected Redshift properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "359404c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_16708\\2240008960.py:2: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth',-1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ClusterIdentifier</td>\n",
       "      <td>my-first-redshift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NodeType</td>\n",
       "      <td>dc2.large</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ClusterStatus</td>\n",
       "      <td>deleting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MasterUsername</td>\n",
       "      <td>awsuser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DBName</td>\n",
       "      <td>myfirstdb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Endpoint</td>\n",
       "      <td>{'Address': 'my-first-redshift.c3igl3ulf3d3.ap-south-1.redshift.amazonaws.com', 'Port': 5439}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>VpcId</td>\n",
       "      <td>vpc-0b364b0263dd27367</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 key  \\\n",
       "0  ClusterIdentifier   \n",
       "1  NodeType            \n",
       "2  ClusterStatus       \n",
       "3  MasterUsername      \n",
       "4  DBName              \n",
       "5  Endpoint            \n",
       "6  VpcId               \n",
       "\n",
       "                                                                                           Value  \n",
       "0  my-first-redshift                                                                              \n",
       "1  dc2.large                                                                                      \n",
       "2  deleting                                                                                       \n",
       "3  awsuser                                                                                        \n",
       "4  myfirstdb                                                                                      \n",
       "5  {'Address': 'my-first-redshift.c3igl3ulf3d3.ap-south-1.redshift.amazonaws.com', 'Port': 5439}  \n",
       "6  vpc-0b364b0263dd27367                                                                          "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prettyRedshiftProps(props):\n",
    "    pd.set_option('display.max_colwidth',-1)\n",
    "    keysToShow = [\"ClusterIdentifier\",\"NodeType\",\"ClusterStatus\",\"MasterUsername\",\"DBName\",\"Endpoint\",\"VpcId\" ]\n",
    "    x =[(k,v) for k , v in props.items() if k in keysToShow]\n",
    "    return pd.DataFrame(data=x, columns=[\"key\",\"Value\"])\n",
    "\n",
    "\n",
    "myClusterprops = redshift_client.describe_clusters( ClusterIdentifier=DWH_CLUSTER_IDENTIFIER)['Clusters'][0]\n",
    "prettyRedshiftProps(myClusterprops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f9981b",
   "metadata": {},
   "source": [
    "##### Extract relevant cluster information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5bf080cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "DWH_Endpoint = myClusterprops['Endpoint']['Address']\n",
    "DWH_Role_ARN = myClusterprops['IamRoles'][0]['IamRoleArn']\n",
    "DWH_DBName = myClusterprops['DBName']\n",
    "DWH_MasterUsername = myClusterprops['MasterUsername']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb636687",
   "metadata": {},
   "source": [
    "##### Configure security group for Redshift cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e3d657af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ec2.SecurityGroup(id='sg-0f55abca3022809b9')\n",
      "Error creating cluster: An error occurred (UnauthorizedOperation) when calling the AuthorizeSecurityGroupIngress operation: You are not authorized to perform this operation. User: arn:aws:iam::533267202542:user/Abhinav_dE_dev is not authorized to perform: ec2:AuthorizeSecurityGroupIngress on resource: arn:aws:ec2:ap-south-1:533267202542:security-group/sg-0f55abca3022809b9 because no identity-based policy allows the ec2:AuthorizeSecurityGroupIngress action. Encoded authorization failure message: r_a4YbMMoPEQ9drlFtqOUOm0a8POqesChGwScfhD3Y29COTFj1kEwSKaDqIXdXTD7U3UYSOBDnMOJMWf9ORMkcL-DerWESDGC69XDP26h0_iQYHz01lqCPpFIzhpk_pWZb_uxsCgzHRJa7bSZkvfyPLKXcVGD3Jsk8Ocrgsj_PtEFiWM-F1MqFBdOkgwiEb8TxKnI0JGR269nTWGa9sy2pXpLBf8s0zQ_JW-OOulrqke9H0oqbchiUQia07RhnmXF8qbBmjdeMaQTazWUvup_nvCB-cG5WiXF8lfRH6YtuQXPzmK2-R5xjeRqXhlBtw0NiYAjNxiVWb-6kw0rB08sQrCdeszet_SGL5qvApF-SbdvT-PB-70PSOLVpLHi1cMWSZY6JkE1kBebjAhFqaB7SZDwcmZgWNKD6DmaMg_vXIpLn1z0wnwlvUCgRgac6UrZ6PP9Lv8tWe9y0AOBJeQGqY2feoq7C6xI4qPokBZbNgrVDSNqm5w6rfjeWpILp9Rtgi2VfnxHticUj43HKb4xNKandPbz42U43bKVA_erVKCKnY2ymnkZHA8-yWFjadum1N9pHm4KwlJFS_CoHo0G9xTnWZOuvL0scUbedOslxBAUQ\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    vpc = ec2.Vpc(id=myClusterprops['VpcId'])\n",
    "    defaultSg = list(vpc.security_groups.all())[0]\n",
    "    print(defaultSg)\n",
    "    \n",
    "    defaultSg.authorize_ingress(\n",
    "        GroupName=defaultSg.group_name,\n",
    "        CidrIp='0.0.0.0/0',\n",
    "        IpProtocol='TCP',\n",
    "        FromPort=int(DWH_PORT),\n",
    "        ToPort=int(DWH_PORT)\n",
    "    )\n",
    "except Exception as e:\n",
    "   \n",
    "    print(f\"Error creating cluster: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd919a8",
   "metadata": {},
   "source": [
    "##### Connect to Redshift cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a966579d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    conn = psycopg2.connect(host=DWH_Endpoint, dbname=DWH_DBName, user=DWH_DB_USER, password=DWH_DB_PASSWORD, port=5439)\n",
    "except Exception as e:\n",
    "    print(f\"Error creating cluster: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4a1eee",
   "metadata": {},
   "source": [
    "##### Set autocommit for the connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c2fffb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.set_session(autocommit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06abccdd",
   "metadata": {},
   "source": [
    "##### Create cursor for executing SQL commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c92a7e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cur = conn.cursor()\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674e32e7",
   "metadata": {},
   "source": [
    "##### Function to generate DDL statement for a specific file in S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7c804a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE Statement 1:\n",
      "CREATE TABLE Customer (\"customer_code\" varchar, \"custmer_name\" varchar, \"customer_type\" varchar);\n",
      "==================================================\n",
      "\n",
      "CREATE TABLE Statement 2:\n",
      "CREATE TABLE Sales_date (\"date\" varchar, \"cy_date\" varchar, \"year\" varchar, \"month_name\" varchar, \"date_yy_mmm\" varchar);\n",
      "==================================================\n",
      "\n",
      "CREATE TABLE Statement 3:\n",
      "CREATE TABLE markets (\"markets_code\" varchar, \"markets_name\" varchar, \"zone\" varchar);\n",
      "==================================================\n",
      "\n",
      "CREATE TABLE Statement 4:\n",
      "CREATE TABLE products (\"product_code\" varchar, \"product_type\" varchar);\n",
      "==================================================\n",
      "\n",
      "CREATE TABLE Statement 5:\n",
      "CREATE TABLE transactions (\"product_code\" varchar, \"customer_code\" varchar, \"market_code\" varchar, \"order_date\" varchar, \"sales_qty\" varchar, \"sales_amount\" varchar, \"currency\" varchar, \"profit_margin_percentage\" varchar, \"profit_margin\" varchar, \"cost_price\" varchar);\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to generate CREATE TABLE statements for CSV files\n",
    "def generate_create_table_statements(bucket_name, file_names):\n",
    "    create_table_statements = []\n",
    "\n",
    "    for file_name in file_names:\n",
    "        try:\n",
    "            s3_object = s3.Object(bucket_name, file_name)\n",
    "            df = pd.read_csv(s3_object.get()['Body'], sep=',')  # Specify the correct delimiter as per your file/ csv\n",
    "            \n",
    "            # Generate CREATE TABLE statement based on DataFrame columns\n",
    "            table_name = file_name.split('/')[-1].split('.')[0]  # Extract table name from file name\n",
    "            columns_definition = \", \".join([f'\"{col}\" varchar' for col in df.columns])\n",
    "            create_table_statement = f\"CREATE TABLE {table_name} ({columns_definition});\"\n",
    "            \n",
    "            # Append CREATE TABLE statement to the list\n",
    "            create_table_statements.append(create_table_statement)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {file_name}: {e}\")\n",
    "    \n",
    "    return create_table_statements\n",
    "\n",
    "# Call the function to get CREATE TABLE statements for each CSV file in S3\n",
    "create_table_statements = generate_create_table_statements(\"abhinav-de-s3\", log_data_files)\n",
    "\n",
    "# Display CREATE TABLE statements\n",
    "for idx, create_table_statement in enumerate(create_table_statements, start=1):\n",
    "    print(f\"CREATE TABLE Statement {idx}:\\n{create_table_statement}\\n{'='*50}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cf733101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Customer.csv',\n",
       " 'Sales_date.csv',\n",
       " 'markets.csv',\n",
       " 'products.csv',\n",
       " 'transactions.csv']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_data_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3822bfe1",
   "metadata": {},
   "source": [
    "##### Execute SQL commands to create a table, copy data, and query the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bec02cf",
   "metadata": {},
   "source": [
    "###### Customer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eada875d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE TABLE Customer (\"customer_code\" varchar, \"custmer_name\" varchar, \"customer_type\" varchar);\n",
    "    \"\"\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0afc4dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cur.execute(\"\"\"\n",
    "        copy Customer from 's3://abhinav-de-s3/Customer.csv' \n",
    "        credentials 'aws_iam_role=arn:aws:iam::533267202542:role/redshift-s3-role'\n",
    "        delimiter ','\n",
    "        region 'ap-south-1'\n",
    "    \"\"\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefe0345",
   "metadata": {},
   "source": [
    "######  Sales_date.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f3aca25",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cur.execute(\"\"\"\n",
    "        CREATE TABLE Sales_date (\"cy_date\" varchar, \"date\" varchar, \"date_yy_mmm\" varchar, \"month_name\" varchar, \"year\" varchar);\n",
    "    \"\"\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d340453",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cur.execute(\"\"\"\n",
    "        copy Sales_date from 's3://abhinav-de-s3/Sales_date.csv' \n",
    "        credentials 'aws_iam_role=arn:aws:iam::533267202542:role/redshift-s3-role'\n",
    "        delimiter ','\n",
    "        region 'ap-south-1'\n",
    "    \"\"\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1db0a9e",
   "metadata": {},
   "source": [
    "###### markets.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a0c7f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cur.execute(\"\"\"\n",
    "   CREATE TABLE markets (\"markets_code\" varchar, \"markets_name\" varchar, \"zone\" varchar);\n",
    "    \"\"\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "63f64e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cur.execute(\"\"\"\n",
    "        copy markets from 's3://abhinav-de-s3/markets.csv' \n",
    "        credentials 'aws_iam_role=arn:aws:iam::533267202542:role/redshift-s3-role'\n",
    "        delimiter ','\n",
    "        region 'ap-south-1'\n",
    "    \"\"\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf008bb",
   "metadata": {},
   "source": [
    "###### products.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c5d2e2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cur.execute(\"\"\"\n",
    "   CREATE TABLE products (\"product_code\" varchar, \"product_type\" varchar);\n",
    "   \"\"\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4e40cdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cur.execute(\"\"\"\n",
    "        copy products from 's3://abhinav-de-s3/products.csv' \n",
    "        credentials 'aws_iam_role=arn:aws:iam::533267202542:role/redshift-s3-role'\n",
    "        delimiter ','\n",
    "        region 'ap-south-1'\n",
    "    \"\"\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a708f357",
   "metadata": {},
   "source": [
    "###### transactions.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bbb74f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cur.execute(\"\"\"\n",
    "  CREATE TABLE transactions (\"product_code\" varchar, \"customer_code\" varchar, \"market_code\" varchar, \"order_date\" varchar, \"sales_qty\" varchar, \"sales_amount\" varchar, \"currency\" varchar, \"profit_margin_percentage\" varchar, \"profit_margin\" varchar, \"cost_price\" varchar);\n",
    "  \"\"\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "080be7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cur.execute(\"\"\"\n",
    "        copy transactions from 's3://abhinav-de-s3/transactions.csv' \n",
    "        credentials 'aws_iam_role=arn:aws:iam::533267202542:role/redshift-s3-role'\n",
    "        delimiter ','\n",
    "        region 'ap-south-1'\n",
    "    \"\"\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce638ad",
   "metadata": {},
   "source": [
    "###### select the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "148478e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cur.execute(\"\"\"\n",
    "        select * from transactions;\n",
    "    \"\"\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a9a44354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Prod279', 'Cus020', 'Mark011', '2017-10-18', '1', '102.0', 'INR', '-0.12', '-12.24', '114.24')\n",
      "('Prod279', 'Cus020', 'Mark011', '2017-10-18', '1', '102.0', 'INR', '-0.12', '-12.24', '114.24')\n",
      "('Prod279', 'Cus020', 'Mark011', '2017-10-18', '1', '102.0', 'INR', '-0.12', '-12.24', '114.24')\n"
     ]
    }
   ],
   "source": [
    "# Fetch and print the first few rows from the result set\n",
    "row = cur.fetchone()\n",
    "i=0\n",
    "while row:\n",
    "    print(row)\n",
    "    i=i+1\n",
    "    if i>2:\n",
    "        break \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb05628",
   "metadata": {},
   "source": [
    "###### Close the connection to the Redshift cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "d33991c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     conn.close()\n",
    "# except Exception as e:\n",
    "#     print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30024547",
   "metadata": {},
   "source": [
    "##### Delete the Redshift cluster if  Redshift cluster is not required any more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eaabc938",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    redshift_client.delete_cluster(ClusterIdentifier=DWH_CLUSTER_IDENTIFIER, SkipFinalClusterSnapshot=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e73c1e66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'myfirstdb'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DWH_DBName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d432147d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'awsuser'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DWH_MasterUsername"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4b4581da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Passw0rd123'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DWH_DB_PASSWORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb70fc14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
